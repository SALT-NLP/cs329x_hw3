{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a5b36d0",
   "metadata": {},
   "source": [
    "# HW3: Building and Evaluating Human-AI Interaction\n",
    "\n",
    "In the previous homeworks you learned about ways of training LLMs to build both general and personalized chat assistants.\n",
    "\n",
    "Once we have AI assistants we can plug them into novel tools for people to use. However building and evaluating such scaffolds for human-AI interaction is a non-trivial task in itself. In this assignment you will go through the process of setting up a Human-AI interaction environment (CoGym), evaluate the AI behaviors manually, and then build automated evaluators for these agents using ideas from AutoMetrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cea598",
   "metadata": {},
   "source": [
    "## Part 1: Building Human-AI Interaction with CoGym\n",
    "\n",
    "Collaborative Gym (CoGym) is a framework for developing and evaluating human–AI collaboration. It provides shared task environments, like travel planning, data analysis, and writing, where both the human and the agent can act at any time instead of taking strict turns. This setup makes it possible to study how agents communicate, coordinate, and share initiative with humans. CoGym includes both simulated and real-world conditions, and its evaluation suite measures not only task success and quality but also how well the collaboration itself worked. The goal is to build AI systems that act as capable teammates rather than passive tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fd905e",
   "metadata": {},
   "source": [
    "# ![Figure 1: Collaborative Gym (Co-Gym) enables collaboration between humans and LM agents within a task environment. Left: Human adds requests and sends multiple messages without waiting for agent responses. Right: Human rates collaboration highly as the agent proactively seeks help when uncertain about package installation.](img/cogym-figure1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c117dcd0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "CoGym runs both the human and the AI agent inside a shared environment that handles actions, messages, and updates through an event-based notification system. This design allows real-time, non-turn-taking coordination: either side can act, edit, or message at any point, and both see synchronized updates. For more information [read the paper](https://arxiv.org/abs/2412.15701).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e6efe3",
   "metadata": {},
   "source": [
    "### Setting up CoGym\n",
    "\n",
    "Follow these instructions to get CoGym running on your local machine:\n",
    "\n",
    "#### Repo and Environment Setup\n",
    "\n",
    "1. Clone cogym onto your local machine: https://github.com/SALT-NLP/collaborative-gym\n",
    "\n",
    "2. Run the following commands in the root of the repo\n",
    "\n",
    "```bash\n",
    "conda create --name cogym python=3.12\n",
    "conda activate cogym\n",
    "pip install -r requirements.txt\n",
    "pip install -U litellm\n",
    "pip install uvicorn\n",
    "```\n",
    "\n",
    "3. Ensure that you have [docker installed](https://docs.docker.com/engine/install/) AND that it is **currently running**.\n",
    "\n",
    "#### API Access and Keys\n",
    "\n",
    "1. Navigate to your Google Cloud project for the course. You will need to enable the following APIs:\n",
    "\n",
    "- Vertex AI ([here](https://console.cloud.google.com/vertex-ai/dashboard))\n",
    "- Google Maps Platform APIs & Services ([here](https://console.cloud.google.com/google/maps-apis/api-list))\n",
    "  - For this enable Distance Matrix API, Places API, Places API (new). **NO NEED** to enable Places Aggregate API or Places UI Kit\n",
    "- Custom Search API ([here](https://console.cloud.google.com/marketplace/product/google/customsearch.googleapis.com))\n",
    "\n",
    "2. Rename the `secrets.example.toml` file to `secrets.toml`\n",
    "\n",
    "3. Now get your API key for the google project [here](https://console.cloud.google.com/apis/credentials). Fill this in as the `GEMINI_API_KEY`, `GOOGLE_MAP_API_KEY`, and `GOOGLE_SEARCH_API_KEY`.\n",
    "\n",
    "4. Create a Custom Search Engine and copy the CX ID ([here](https://cse.google.com/cse)). Fill this in as `GOOGLE_CSE_ID`.\n",
    "\n",
    "#### Editing the codebase and running the backend server\n",
    "\n",
    "1. Open to `collaborative_gym/server.py`.\n",
    "\n",
    "Change lines 152-154 from\n",
    "\n",
    "```python\n",
    "                            \"demo_agent.collaborative_agent_with_situational_planning.agent \"\n",
    "                            \"--model-name gpt-4o --wait-time 1 --enhance-user-control\",\n",
    "```\n",
    "\n",
    "to\n",
    "\n",
    "```python\n",
    "                            \"demo_agent.basic_collaborative_agent.agent \"\n",
    "                            \"--model-name gemini/gemini-pro-latest --wait-time 1 --enhance-user-control\",\n",
    "```\n",
    "\n",
    "Note that gemini-pro-latest will work best, but run much slower than `gemini-flash-latest`.  If runtime is a problem for you consider changing to flash!\n",
    "\n",
    "2. change your current directory to the `cs329x/collaborative-gym` folder\n",
    "\n",
    "3. Setup the redis server (make sure docker is running!)\n",
    "\n",
    "```bash\n",
    "docker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest\n",
    "```\n",
    "\n",
    "4. Launch the backend server (make sure nothing is already running on localhost:8000)\n",
    "\n",
    "```bash\n",
    "DISABLE_AGENT=false uvicorn collaborative_gym.server:app --reload\n",
    "```\n",
    "\n",
    "#### Setting up the frontend\n",
    "\n",
    "1. Leave the backend and redis servers running (Open up a new terminal window)\n",
    "\n",
    "2. Navigate to `frontend/workbench`\n",
    "\n",
    "3. create a file `.env.local` with the following contents\n",
    "\n",
    "```\n",
    "NEXT_PUBLIC_USE_MOCK_API=\"false\"\n",
    "NEXT_PUBLIC_API_URL=\"http://localhost:8000/api\"\n",
    "NEXT_PUBLIC_WS_URL=\"ws://localhost:8000/ws\"\n",
    "```\n",
    "\n",
    "4. Install the necessary packages ([install npm](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) if you don't already have it)\n",
    "\n",
    "```bash\n",
    "npm install -g pnpm\n",
    "pnpm install\n",
    "```\n",
    "\n",
    "5. Run the frontend\n",
    "\n",
    "```bash\n",
    "pnpm run dev\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afe78e8",
   "metadata": {},
   "source": [
    "### Running CoGym (25 points)\n",
    "\n",
    "1. Open up the CoGym Web Interface (likely [localhost:3000](localhost:3000))\n",
    "\n",
    "2. Start writing a Travel Plan with the Agent for a trip you might want to do someday.\n",
    "\n",
    "3. Interact with the Agent to produce a travel plan.\n",
    "\n",
    "![Interacting with the cogym agent](img/cogym-example.png)\n",
    "\n",
    "4. When you are done click `Finish` in the upper right hand corner. This will give you a chance to copy the travel plan you created.\n",
    "\n",
    "#### Question 1.1 (15 points)\n",
    "\n",
    "Produce 3 travel plans with cogym! Please be creative and create travel plans that are actually interesting to you or have interesting constraints (budget, activities, multi-city, etc.) When you are done working with the collaborative agent paste your completed travel plans to `writeup.md`.\n",
    "\n",
    "#### Question 1.2 (5 points)\n",
    "\n",
    "Evaluate these travel plans. Which one is the best? Which was the worst? Score each on quality from (1-5) using your own criteria. (We are looking for a score of 1-5 for each travel plan). Add this to `writeup.md`.\n",
    "\n",
    "#### Question 1.3 (5 points)\n",
    "\n",
    "Explain what criteria you were using when doing this evaluation. What mattered? (2-3 sentences). Add this to `writeup.md`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d62efce",
   "metadata": {},
   "source": [
    "If you found working on builing human-ai interactions interesting you can reach out to [shaoyj@stanford.edu](mailto:shaoyj@stanford.edu) for more in this space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665b0c9c",
   "metadata": {},
   "source": [
    "## Part 2: Building Automatic Evaluators to Approximate Human Judgement (50 points + 10 extra credit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bba3ca3",
   "metadata": {},
   "source": [
    "You just got a sense of how human evaluation can be a subjective process. Still, when designing research projects or products it is important to be able to quantify some of this fuzzy qualitative signals. For this we need metrics.\n",
    "\n",
    "CoGym directly introduces a few metrics. For outcomes, it measures Delivery Rate (whether the task was completed) and Task Performance (the quality of the final result).\n",
    "\n",
    "Now you are going to try to write an automatic evaluator. As a start, let's write an LLM as a Judge prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce83956",
   "metadata": {},
   "source": [
    "#### Helper Functions and Imports (Do not modify)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aa6053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from dotenv import load_dotenv\n",
    "import litellm\n",
    "from litellm import disable_cache, enable_cache\n",
    "from litellm.caching.caching import Cache\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Callable, Optional\n",
    "\n",
    "litellm.cache = Cache(type=\"disk\")\n",
    "\n",
    "litellm.set_verbose = False\n",
    "litellm.suppress_debug_info = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d5c71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)  # show full text in each cell\n",
    "pd.set_option(\"display.width\", 2000)  # prevent horizontal truncation\n",
    "pd.set_option(\"display.max_columns\", None)  # show all columns\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a4f0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt=\"\", model=\"gemini/gemini-2.0-flash\"):\n",
    "    if type(prompt) == str:\n",
    "        prompt = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    return (\n",
    "        litellm.completion(\n",
    "            model=model,\n",
    "            messages=prompt,\n",
    "        )\n",
    "        .choices[0]\n",
    "        .message.content\n",
    "    )\n",
    "\n",
    "\n",
    "def _llm_as_judge_prompt(prompt, model=\"gemini/gemini-flash-latest\"):\n",
    "    system_prompt = \"\"\"You are a expert evaluator.  Given the criteria that you are evaluating for, you will score the given response from 1-5.\n",
    "\n",
    "First, reason over the given response and how it does or does not meet the criteria.  Then, give your final score.\n",
    "\n",
    "Follow the following format:\n",
    "\n",
    "<Reasoning>\n",
    "[Your reasoning which will help you come up with your score]\n",
    "</Reasoning>\n",
    "<Score>\n",
    "[Your final score; 1-5]\"\"\"\n",
    "\n",
    "    if type(prompt) == str:\n",
    "        prompt = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    prompt = [{\"role\": \"system\", \"content\": system_prompt}] + prompt\n",
    "\n",
    "    try:\n",
    "        response = llm(prompt, model)\n",
    "\n",
    "        # Extract score from response (the next line after <Score>)\n",
    "        score_line = response.split(\"<Score>\")[1].split(\"</Score>\")[0].strip()\n",
    "        score = float(score_line)\n",
    "\n",
    "        return score\n",
    "    except Exception as e:\n",
    "        prompt[-1][\"content\"] = (\n",
    "            prompt[-1][\"content\"]\n",
    "            + \"\\n\\nBe very careful and precise in formatting your response.\"\n",
    "        )\n",
    "        res2 = llm(prompt)\n",
    "        score_line = res2.split(\"<Score>\")[1].split(\"</Score>\")[0].strip()\n",
    "        score = float(score_line)\n",
    "\n",
    "        return score\n",
    "\n",
    "\n",
    "def llm_as_judge(\n",
    "    evaluation_criteria, input, output, model=\"gemini/gemini-flash-latest\"\n",
    "):\n",
    "    prompt = f\"\"\"<Evaluation Criteria>\n",
    "    {evaluation_criteria}\n",
    "    </Evaluation Criteria>\n",
    "    <Input provided to the AI>\n",
    "    {input}\n",
    "    </Input>\n",
    "    <Output to evaluate>\n",
    "    {output}\n",
    "    </Output>\n",
    "    \"\"\"\n",
    "    return _llm_as_judge_prompt(prompt, model)\n",
    "\n",
    "\n",
    "def run_llm_as_judge_on_df(\n",
    "    df, evaluation_criteria, model=\"gemini/gemini-flash-latest\", num_threads=16\n",
    "):\n",
    "    scores = [None] * len(df)\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = {\n",
    "            executor.submit(\n",
    "                llm_as_judge,\n",
    "                evaluation_criteria,\n",
    "                row.input,\n",
    "                row.output,\n",
    "                model,\n",
    "            ): i\n",
    "            for i, row in enumerate(df.itertuples(index=False))\n",
    "        }\n",
    "\n",
    "        for future in tqdm(\n",
    "            as_completed(futures), total=len(futures), desc=\"Scoring rows\"\n",
    "        ):\n",
    "            i = futures[future]\n",
    "            scores[i] = future.result()\n",
    "    return scores\n",
    "\n",
    "\n",
    "def run_regression_judge_on_df(\n",
    "    df,\n",
    "    criteria_list,\n",
    "    coef,\n",
    "    intercept,\n",
    "    model=\"gemini/gemini-flash-latest\",\n",
    "    num_threads=16,\n",
    "):\n",
    "    for criterion in criteria_list:\n",
    "        scores = run_llm_as_judge_on_df(df, criterion, model, num_threads)\n",
    "        df[criterion] = scores\n",
    "\n",
    "    coef_arr = np.asarray(coef).reshape(-1)  # force (n_features,)\n",
    "\n",
    "    df[\"predicted_score\"] = df[criteria_list].dot(coef_arr) + intercept\n",
    "\n",
    "    return df[\"predicted_score\"].tolist()\n",
    "\n",
    "\n",
    "def run_evaluator_on_df(\n",
    "    df, evaluator: Callable[[str, str], float], num_threads=16\n",
    ") -> list[float]:\n",
    "    # Preallocate result list\n",
    "    scores = [None] * len(df)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        # Submit one task per row\n",
    "        futures = {\n",
    "            executor.submit(evaluator, row.input, row.output): i\n",
    "            for i, row in enumerate(df.itertuples(index=False))\n",
    "        }\n",
    "\n",
    "        # Collect results as they complete\n",
    "        for future in tqdm(\n",
    "            as_completed(futures), total=len(futures), desc=\"Scoring rows\"\n",
    "        ):\n",
    "            i = futures[future]\n",
    "            scores[i] = future.result()\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90c0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pearson_correlation(df, scores: list[float]) -> float:\n",
    "    y_true = df[\"score\"]\n",
    "    y_pred = pd.Series(scores, index=df.index, name=\"predicted_score\")\n",
    "\n",
    "    paired = pd.concat([y_true, y_pred], axis=1).dropna()\n",
    "    if len(paired) < 2:\n",
    "        return np.nan\n",
    "    if paired.iloc[:, 0].nunique() < 2 or paired.iloc[:, 1].nunique() < 2:\n",
    "        return np.nan  # zero variance on either side\n",
    "\n",
    "    return paired.iloc[:, 0].corr(paired.iloc[:, 1], method=\"pearson\")\n",
    "\n",
    "def get_judge_confidence_interval(\n",
    "    df,\n",
    "    evaluation_criteria,\n",
    "    model=\"gemini/gemini-flash-latest\",\n",
    "    num_threads=16,\n",
    "    trials=5,\n",
    "):\n",
    "    disable_cache()\n",
    "    scores = []\n",
    "    for _ in range(trials):\n",
    "        print(f\"Running trial {_ + 1} of {trials}\")\n",
    "        scores.append(\n",
    "            run_llm_as_judge_on_df(df, evaluation_criteria, model, num_threads)\n",
    "        )\n",
    "    enable_cache()\n",
    "\n",
    "    pearson_correlations = []\n",
    "    for score_list in scores:\n",
    "        corr = compute_pearson_correlation(df, score_list)\n",
    "        if not np.isnan(corr):\n",
    "            pearson_correlations.append(corr)\n",
    "        else:\n",
    "            trials -= 1\n",
    "    return np.array(pearson_correlations).mean(), np.array(\n",
    "        pearson_correlations\n",
    "    ).std() / np.sqrt(trials)\n",
    "\n",
    "\n",
    "def get_regression_confidence_interval(\n",
    "    df,\n",
    "    criteria_list,\n",
    "    coef,\n",
    "    intercept,\n",
    "    model=\"gemini/gemini-flash-latest\",\n",
    "    num_threads=16,\n",
    "    trials=5,\n",
    "):\n",
    "    disable_cache()\n",
    "    scores = []\n",
    "    for _ in range(trials):\n",
    "        print(f\"Running trial {_ + 1} of {trials}\")\n",
    "        scores.append(\n",
    "            run_regression_judge_on_df(\n",
    "                df, criteria_list, coef, intercept, model, num_threads\n",
    "            )\n",
    "        )\n",
    "    enable_cache()\n",
    "\n",
    "    pearson_correlations = []\n",
    "    for score_list in scores:\n",
    "        pearson_correlations.append(compute_pearson_correlation(df, score_list))\n",
    "    return np.array(pearson_correlations).mean(), np.array(\n",
    "        pearson_correlations\n",
    "    ).std() / np.sqrt(trials)\n",
    "\n",
    "\n",
    "def get_evaluator_confidence_interval(\n",
    "    df, evaluator: Callable[[str, str], float], num_threads=16, trials=5\n",
    ") -> tuple[float, float]:\n",
    "    disable_cache()\n",
    "    scores = []\n",
    "    for _ in range(trials):\n",
    "        scores.append(run_evaluator_on_df(df, evaluator, num_threads))\n",
    "    enable_cache()\n",
    "\n",
    "    pearson_correlations = []\n",
    "    for score_list in scores:\n",
    "        pearson_correlations.append(compute_pearson_correlation(df, score_list))\n",
    "    return np.array(pearson_correlations).mean(), np.array(\n",
    "        pearson_correlations\n",
    "    ).std() / np.sqrt(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8561969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_df_set(\n",
    "    train_df_list,\n",
    "    test_df_list,\n",
    "    evaluator_constructor: Callable[[pd.DataFrame, str], Callable[[str, str], float]],\n",
    "    human_criteria_list: Optional[list[str]] = None,\n",
    "    num_threads: int = 16,\n",
    "    trials: int = 5,\n",
    "    parallel: bool = False,\n",
    "):\n",
    "    if human_criteria_list is None:\n",
    "        human_criteria_list = [\"Unknown\"] * len(train_df_list)\n",
    "\n",
    "    # package args per task\n",
    "    task_args = list(zip(train_df_list, test_df_list, human_criteria_list))\n",
    "\n",
    "    def run_one(train_df, test_df, human_criteria):\n",
    "        evaluator = evaluator_constructor(train_df, human_criteria)\n",
    "        mean, std = get_evaluator_confidence_interval(\n",
    "            test_df,\n",
    "            evaluator,\n",
    "            num_threads=num_threads,\n",
    "            trials=trials,\n",
    "        )\n",
    "        return (mean, std)\n",
    "\n",
    "    if parallel:\n",
    "        with ThreadPoolExecutor(max_workers=num_threads) as ex:\n",
    "            results = list(ex.map(lambda args: run_one(*args), task_args))\n",
    "    else:\n",
    "        results = [run_one(*args) for args in task_args]\n",
    "\n",
    "    # side-effect print, same as before\n",
    "    for mean, std in results:\n",
    "        print(f\"Pearson correlation: {mean} ± {std}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        dev_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        human_criteria: str = \"Unknown\",\n",
    "    ):\n",
    "        self.train_df = train_df\n",
    "        self.dev_df = dev_df\n",
    "        self.test_df = test_df\n",
    "        self.human_criteria = human_criteria\n",
    "\n",
    "\n",
    "def init_dataset(path: str) -> Dataset:\n",
    "    train_df = pd.read_csv(f\"{path}/train.csv\")\n",
    "    dev_df = pd.read_csv(f\"{path}/val.csv\")\n",
    "    test_df = pd.read_csv(f\"{path}/test.csv\")\n",
    "    with open(f\"{path}/human_criteria.txt\", \"r\") as f:\n",
    "        human_criteria = f.read().strip()\n",
    "    return Dataset(train_df, dev_df, test_df, human_criteria)\n",
    "\n",
    "\n",
    "def evaluate_on_datasets(\n",
    "    datasets: list[Dataset],\n",
    "    evaluator_constructor: Callable[[pd.DataFrame, str], Callable[[str, str], float]],\n",
    "    num_threads=16,\n",
    "    trials=5,\n",
    "):\n",
    "    return evaluate_on_df_set(\n",
    "        [dataset.train_df for dataset in datasets],\n",
    "        [dataset.test_df for dataset in datasets],\n",
    "        evaluator_constructor,\n",
    "        [dataset.human_criteria for dataset in datasets],\n",
    "        num_threads,\n",
    "        trials,\n",
    "    )\n",
    "\n",
    "\n",
    "cogym = init_dataset(\"datasets/CoGym\")\n",
    "helpsteer2 = init_dataset(\"datasets/HelpSteer2\")\n",
    "simpeval = init_dataset(\"datasets/SimpEval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5fc635",
   "metadata": {},
   "source": [
    "### Question 2.1: Writing your own LLM-as-a-Judge Evaluator (10 points)\n",
    "\n",
    "Take a look at some of the CoGym data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f530ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cogym.test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f25a493",
   "metadata": {},
   "source": [
    "Now devise a prompt that works to make a more reliable automatic evaluator of travel plan quality. You will want to use the following functions:\n",
    "\n",
    "- `run_llm_as_judge_on_df(df, prompt):` - runs an llm judge in parallel over a dataframe and returns a list of scores for each row\n",
    "- `compute_pearson_correlation(df, scores):` - computes the pearson correlation of LLM judge scores versus the true human labels on a dataframe\n",
    "\n",
    "Please try out a few prompts. Note that due to the small test set size there will be more variance in your correlations. For more reliable assessment we provide the `get_judge_confidence_interval(df, prompt)` method which runs five trials.\n",
    "\n",
    "Note that the LLM Judge is automatically cached so several runs with the same prompt will yield the same result. `get_judge_confidence_interval` purposefully disables the cache.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6163cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"Assess the travel plan for quality.\"\"\"  # TODO: Write your prompt here!  You may choose to write a much longer prompt with more criteria.\n",
    "\n",
    "scores = run_llm_as_judge_on_df(cogym.test_df, PROMPT)\n",
    "\n",
    "print(scores)\n",
    "print(compute_pearson_correlation(cogym.test_df, scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ce1c33",
   "metadata": {},
   "source": [
    "Now that you have played around with some prompts you should have a sense of the challenge of writing this evaluation criteria.\n",
    "\n",
    "Please use the `get_judge_confidence_interval` method to test your prompt for reliablility. For full credit find a prompt that achieves MEAN correlation >= 0.15 and STDDEV <= 0.08.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec3c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = get_judge_confidence_interval(cogym.test_df, PROMPT)\n",
    "print()\n",
    "print(f\"Pearson correlation: {mean} ± {std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d9ee5a",
   "metadata": {},
   "source": [
    "Copy and paste your prompt and Pearson correlation (mean ± stddev) into `writeup.md`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b1cdeb",
   "metadata": {},
   "source": [
    "#### Motivating AutoMetrics\n",
    "\n",
    "Now let's introduce two new datasets: HelpSteer2 and SimpEval.\n",
    "\n",
    "HelpSteer2 asked human annotators to rate conversations on a scale of helpfulness from 1-5.\n",
    "\n",
    "SimpEval asked several annotators rate sentence simplifications on a scale from 1-100.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e3ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(helpsteer2.test_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56d69aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(simpeval.test_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24462a47",
   "metadata": {},
   "source": [
    "We could manually go through and write predictive LLM as a Judge prompts for each of these tasks, however, a more scalable approach would be to automatically discover the insights that matter using an LLM and turning that into evaluation.\n",
    "\n",
    "This is a core insight in the recent AutoMetrics paper. AutoMetrics was motivated by the challenge that human evaluation is slow, expensive, and hard to scale across new tasks. Instead of hand-designing rubrics, AutoMetrics uses a small amount of human feedback to automatically generate and weight evaluation criteria. It does this through a four-step pipeline—generate, retrieve, regress, and report—that creates candidate rubrics with an LLM, filters them with relevant existing metrics, and learns how to combine them to best predict human judgments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743ace93",
   "metadata": {},
   "source": [
    "![AutoMetrics takes you from expensive measures to interpretable automatic metrics.  Here AutoMetrics generates useful metrics for evaluating LLM written product descriptions from user reviews from EvalGen \\citep{10.1145/3654777.3676450}.  Percentages indicate relative importance of each metric derived from regression coefficients.](img/autometrics-figure1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9af356",
   "metadata": {},
   "source": [
    "Here we are going to reproduce two steps in the AutoMetrics method: **criteria generation** and **regression**. We will leave out existing metrics, metric retrieval, and reporting for this assignment.\n",
    "\n",
    "![AutoMetrics comprises four steps. (1) Generate: create task-specific candidate metrics (Single criteria, Rubric, Examples, MIPROv2). (2) Retrieve: from the generated candidates plus MetricBank, use ColBERT to prefilter to $k'$ metric cards and an LLM to select the final $k$. (3) Regress: fit a PLS model on the training set to weight and select metrics that predict human judgments. (4) Report: produce a writeup with weights and correlations and details to guide adoption.](img/autometrics-method.png)\n",
    "\n",
    "Let's start with criteria generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892e670c",
   "metadata": {},
   "source": [
    "### Question 2.2: Automatically Generating Criteria (10 points)\n",
    "\n",
    "In order to automatically generate criteria we ask that you implement the following algorithm:\n",
    "\n",
    "1. Sort the training data based on the human scores into lowest and highest scoring\n",
    "\n",
    "2. Sample 5 of the worst 10 examples and 5 of the best 10 examples. (Use the seed parameter to ensure reproducibility)\n",
    "\n",
    "3. Format these examples in a prompt to an LLM to explain what key differences exist between the quality of these outputs. You can mention in the prompt to the LLM what the annotators were trying to measure.\n",
    "\n",
    "4. Return a list of at least 5 criteria that independently can be used for an LLM as a Judge.\n",
    "\n",
    "You may find it helpful to use the `llm(prompt) -> response` helper function defined above,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e117ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to format examples\n",
    "def format_example(example):\n",
    "    return f\"\"\"<INPUT>\n",
    "    {example['input']}\n",
    "    </INPUT>\n",
    "    <OUTPUT>\n",
    "    {example['output']}\n",
    "    </OUTPUT>\n",
    "    <SCORE>\n",
    "    {example['score']}\n",
    "    </SCORE>\"\"\"\n",
    "\n",
    "\n",
    "def format_examples(examples):\n",
    "    return \"\\n\\n\".join(\n",
    "        [f\"[[{i}]] \" + format_example(example) for i, example in examples.iterrows()]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f198adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate criteria from non-descriptive human feedback\n",
    "# Inputs:\n",
    "#   train_df: a dataframe of training examples with input, output, and score columns\n",
    "#   human_criteria: a string of the human criteria for the task\n",
    "#   seed: an integer for the random seed\n",
    "# Outputs:\n",
    "#   A list of strings of the criteria.\n",
    "def generate_criteria(train_df, human_criteria=\"Unknown\", seed=42) -> list[str]:\n",
    "    # TODO: Implement your code here!\n",
    "\n",
    "    # Sort the training data based on the human scores into lowest and highest scoring\n",
    "\n",
    "    # Sample 5 of the worst 10 examples and 5 of the best 10 examples (use the seed parameter to ensure reproducibility)\n",
    "\n",
    "    # Format examples in a prompt\n",
    "\n",
    "    # Call the LLM to generate the criteria\n",
    "    \n",
    "    # Return the list of criteria\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61712b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria_cogym = generate_criteria(cogym.train_df, \"travel plan quality\")\n",
    "print(criteria_cogym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f679c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria_helpsteer2 = generate_criteria(helpsteer2.train_df, \"helpfulness\")\n",
    "print(criteria_helpsteer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb74f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria_simpeval = generate_criteria(simpeval.train_df, \"simplification quality\")\n",
    "print(criteria_simpeval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca86862",
   "metadata": {},
   "source": [
    "Please paste your implementation of `generate_critera` in `writeup.md` alongside the (minimum) 5 criteria that the LLM generated. It is okay if you generated more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2870acc4",
   "metadata": {},
   "source": [
    "### Question 2.3: Selecting the Criteria (10 points)\n",
    "\n",
    "Now we have a list of criteria that could potentially be useful to inform an LLM as a Judge, but we don't actually know which ones we should use. For that we can actually use a relatively simple tool: regression.\n",
    "\n",
    "In the paper, we use Partial Least Squares (PLS) regression because it works well when the number of metrics (predictors) is similar to or larger than the number of data points, and when those metrics are highly correlated. PLS finds the direction in metric space that best predicts human judgments and assigns weights accordingly. This lets AutoMetrics identify which criteria truly matter, even with limited data, and combine them into a single predictive score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1395a5",
   "metadata": {},
   "source": [
    "Implement the following method:\n",
    "\n",
    "1. Given a list of potential LLM as a Judge criteria, run the LLM judges on the training dataset\n",
    "2. Compute a PLS regression on all of these outputs. You will find scikit-learn's `sklearn.cross_decomposition.PLSRegression` helpful. For now we will just use 1 component.\n",
    "3. Return the regression coefficients as a list of length `n` where `n` is the number of criteria input, and a y-intercept\n",
    "\n",
    "You may find it helpful to use the helper method `run_llm_as_judge_on_df(df, criteria_string) -> list[float]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af7124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "\n",
    "# Run the LLM judges on the training dataset and return the regression coefficients and intercept\n",
    "# Inputs:\n",
    "#   train_df: a dataframe of training examples with input, output, and score columns\n",
    "#   criteria_list: a list of strings of the LLM as a Judge criteria to regress on\n",
    "# Outputs:\n",
    "#   A tuple of the regression coefficients (list[float]) and intercept (float)\n",
    "def regress_criteria(train_df, criteria_list) -> Tuple[List[float], float]:\n",
    "    # TODO: Implement your code here!\n",
    "\n",
    "     # Run the LLM judges on the training dataset\n",
    "\n",
    "    # NOTE: Ensure the shape of the matrix is correct for the PLSRegression\n",
    "\n",
    "    # Fit PLS\n",
    "\n",
    "    # Return the regression coefficients (list[float]) and intercept (float) as a tuple\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d890f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef, inter = regress_criteria(cogym.train_df, criteria_cogym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1ec8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(coef) == len(criteria_cogym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705389a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = get_regression_confidence_interval(\n",
    "    cogym.test_df, criteria_cogym, coef, inter\n",
    ")\n",
    "print(f\"Pearson correlation: {mean} ± {std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eca733a",
   "metadata": {},
   "source": [
    "Paste your code for `regress_criteria` into `writeup.md` alongside the results of `get_regression_confidence_interval`. Ideally your automatically generated criteria can get similar correlation to the Manual LLM Judge prompt you wrote earlier, however this didn't require any manual prompt engineering.  \n",
    "\n",
    "Note: This may not be what you observe because for the purposes of the assignment we are operating on small LLMs (gemini-flash-2.0 instead of gemini-pro-2.5).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b844942",
   "metadata": {},
   "source": [
    "### Question 2.4: Building Better Automatic Evaluators (20 points + 10 extra credit)\n",
    "\n",
    "**[Optional for 3 credit students; Required for 4 credit students]**\n",
    "\n",
    "Now we have learned one way to automatically generate automatic evaluators through generation + regression. There are many other clever strategies you could try. In the paper we also try prompt optimization, finetuning, revising criteria into rubrics, etc.\n",
    "\n",
    "Below we provide you with some seed ideas as well as scaffolding for your evaluator function. Your task is to write an automatic evaluation generator that exceeds a certain threshold on our held out test datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66feb03",
   "metadata": {},
   "source": [
    "#### Ideas you may wish to explore:\n",
    "\n",
    "- Reflective Prompt Optimization: https://arxiv.org/abs/2507.19457\n",
    "- LLM Based Clustering for Criteria extraction: https://stanfordhci.github.io/lloom/ https://arxiv.org/abs/2503.08893\n",
    "- Bootstrap Reasoning Induction: https://arxiv.org/pdf/2203.14465\n",
    "\n",
    "Or come up with something entirely unique!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a6b271",
   "metadata": {},
   "source": [
    "To do this task we ask that you implement `generate_evaluator` below. This method takes in a train_df. It returns a callable function that when passed an input and output it returns a score. You will be graded based on the pearson correlation of your automatic evaluator with human judgements on three held out datasets (not shared with the class). These datasets are similar to the datasets we provided in the earlier parts of this homework, but cover slightly different domains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18ffbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a train_df and human_criteria, generate an automatic evaluator that can be used to score a model's output\n",
    "# Inputs:\n",
    "#   train_df: a dataframe of training examples with input, output, and score columns\n",
    "#   human_criteria: a string of the human criteria for the task (e.g. \"travel plan quality\", \"helpfulness\", \"simplification quality\")\n",
    "# Outputs:\n",
    "#   A callable function that takes an input and output and returns a score\n",
    "def generate_evaluator(\n",
    "    train_df, human_criteria=\"Unknown\"\n",
    ") -> Callable[[str, str], float]:\n",
    "    # TODO: Replace this implementation with your code here!  This implements the generation + regression method from above\n",
    "\n",
    "    # Generate criteria\n",
    "    criteria_list = generate_criteria(train_df, human_criteria)\n",
    "\n",
    "    # Regress criteria\n",
    "    coef, inter = regress_criteria(train_df, criteria_list)\n",
    "\n",
    "    def evaluate(input: str, output: str) -> float:\n",
    "        scores = []\n",
    "        for criterion in criteria_list:\n",
    "            score = llm_as_judge(criterion, input, output)\n",
    "            scores.append(score)\n",
    "        return (coef @ np.array(scores)) + inter\n",
    "\n",
    "    return evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0006aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_on_datasets([cogym, helpsteer2, simpeval], generate_evaluator, trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ecb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results:\")\n",
    "print(\"[Cogym] Pearson correlation: \", results[0][0], \"±\", results[0][1])\n",
    "print(\"[HelpSteer2] Pearson correlation: \", results[1][0], \"±\", results[1][1])\n",
    "print(\"[SimpEval] Pearson correlation: \", results[2][0], \"±\", results[2][1])\n",
    "\n",
    "print(\n",
    "    \"Average Pearson correlation: \", np.array([result[0] for result in results]).mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c5d07a",
   "metadata": {},
   "source": [
    "When you are done implementing your custom method paste your implementation into `writeup.md` AND into `submission.py`. The code in `submission.py` will be run to determine your place on the leaderboard. We will run your evaluator on our held-out test datasets.\n",
    "\n",
    "In order to get full credit you will need to exceed the baseline score (generate + regress) of `0.450` average pearson correlation on our test set. For reference the equivalent score on the provided datasets is `0.439`, so try to beat this locally before submitting.  Additionally the **top 3** submissions on the leaderboard by the homework deadline will get 10 points of extra credit.\n",
    "\n",
    "Finally we ask that you add a writeup of your approach (1-2 paragraphs) to `writeup.md`. If you took any inspiration from papers be sure to link them in your explanation! We look forward to seeing what you come up with!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96111fbb",
   "metadata": {},
   "source": [
    "If you found working on automating evaluations of AI systems interesting you can reach out to [mryan0@stanford.edu](mailto:mryan0@stanford.edu) for more in this space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
